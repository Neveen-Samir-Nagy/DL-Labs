{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment1_part1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HNdarz1uIIZ"
      },
      "source": [
        "<table align=\"center\">\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/KhaledElTahan/DeepLearning/blob/master/Labs/lab1/lab1_part1.ipynb\">\n",
        "        <img src=\"http://introtodeeplearning.com/images/colab/colab.png?v2.0\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
        "</table>\n",
        "\n",
        "# Copyright Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSA7aTOivRhA"
      },
      "source": [
        "**Parts of this lab are based on Kaggle kernels.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfATj8pltlxh"
      },
      "source": [
        "# Lab 1 - Part1: Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOy1mYdKw5TE"
      },
      "source": [
        "![Linear Regression](https://raw.githubusercontent.com/KhaledElTahan/AUC-DeepLearning/master/Labs/lab1/linear_regression.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsLKJ-4kwLYv"
      },
      "source": [
        "## 1.1.1 Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQxlb182u_gv"
      },
      "source": [
        "The problem we are trying to solve here is finding a new house which is suitable to our needs and the budget we assigned. The client who wants to buy the new house did her research and found some houses. She wrote the details of each house she visited including location, sale condition, sale type, house price, among others. She needs some help to know how much she is expected to pay to get a house that conforms with her specific needs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUidwfyPvcfv"
      },
      "source": [
        "Your task is to build a linear regression model that helps her to predict the house price depending on the given attributes she collected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeXilc2Nvg-g"
      },
      "source": [
        "## 1.1.2 Problem Details"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsHAbLgL1MsY"
      },
      "source": [
        "Let's dive into the code, explain it and show you the parts you need to fill!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7n5JS__dwfiH"
      },
      "source": [
        "### 1.1.2.1 Import Needed packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xm9MPDVSsDpu"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import skew\n",
        "from scipy.stats.stats import pearsonr\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.regularizers import l1, l2\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EfciecZwm6L"
      },
      "source": [
        "### 1.1.2.2 Configure Notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EW1TyciwsN9_"
      },
      "source": [
        "%config InlineBackend.figure_format = 'png' #set 'png' here when working on notebook\n",
        "%matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6PJSAiyx4HM"
      },
      "source": [
        "### 1.1.2.3 Work on the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcVsD6V1x-Ty"
      },
      "source": [
        "This dataset contains 80 features that demonstrate the state of the house and our target which is the house price.\n",
        "\n",
        "We begin by loading the train and test splits of the dataset using pandas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6zYNeXVvvRA"
      },
      "source": [
        "train = pd.read_csv(\"https://raw.githubusercontent.com/KhaledElTahan/DeepLearning/master/Labs/lab1/lab1_housing_train.csv\")\n",
        "test = pd.read_csv(\"https://raw.githubusercontent.com/KhaledElTahan/DeepLearning/master/Labs/lab1/lab1_housing_test.csv\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyl1lDq-yVFG"
      },
      "source": [
        "You can have a look at the train split of the dataset using the head command. I very much encourage you to have a deeper look on the dataset file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2JWObzgyY9t",
        "outputId": "c6b27d03-e8ca-4033-e479-0d1785e6e135",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>MSSubClass</th>\n",
              "      <th>MSZoning</th>\n",
              "      <th>LotFrontage</th>\n",
              "      <th>LotArea</th>\n",
              "      <th>Street</th>\n",
              "      <th>Alley</th>\n",
              "      <th>LotShape</th>\n",
              "      <th>LandContour</th>\n",
              "      <th>Utilities</th>\n",
              "      <th>LotConfig</th>\n",
              "      <th>LandSlope</th>\n",
              "      <th>Neighborhood</th>\n",
              "      <th>Condition1</th>\n",
              "      <th>Condition2</th>\n",
              "      <th>BldgType</th>\n",
              "      <th>HouseStyle</th>\n",
              "      <th>OverallQual</th>\n",
              "      <th>OverallCond</th>\n",
              "      <th>YearBuilt</th>\n",
              "      <th>YearRemodAdd</th>\n",
              "      <th>RoofStyle</th>\n",
              "      <th>RoofMatl</th>\n",
              "      <th>Exterior1st</th>\n",
              "      <th>Exterior2nd</th>\n",
              "      <th>MasVnrType</th>\n",
              "      <th>MasVnrArea</th>\n",
              "      <th>ExterQual</th>\n",
              "      <th>ExterCond</th>\n",
              "      <th>Foundation</th>\n",
              "      <th>BsmtQual</th>\n",
              "      <th>BsmtCond</th>\n",
              "      <th>BsmtExposure</th>\n",
              "      <th>BsmtFinType1</th>\n",
              "      <th>BsmtFinSF1</th>\n",
              "      <th>BsmtFinType2</th>\n",
              "      <th>BsmtFinSF2</th>\n",
              "      <th>BsmtUnfSF</th>\n",
              "      <th>TotalBsmtSF</th>\n",
              "      <th>Heating</th>\n",
              "      <th>...</th>\n",
              "      <th>CentralAir</th>\n",
              "      <th>Electrical</th>\n",
              "      <th>1stFlrSF</th>\n",
              "      <th>2ndFlrSF</th>\n",
              "      <th>LowQualFinSF</th>\n",
              "      <th>GrLivArea</th>\n",
              "      <th>BsmtFullBath</th>\n",
              "      <th>BsmtHalfBath</th>\n",
              "      <th>FullBath</th>\n",
              "      <th>HalfBath</th>\n",
              "      <th>BedroomAbvGr</th>\n",
              "      <th>KitchenAbvGr</th>\n",
              "      <th>KitchenQual</th>\n",
              "      <th>TotRmsAbvGrd</th>\n",
              "      <th>Functional</th>\n",
              "      <th>Fireplaces</th>\n",
              "      <th>FireplaceQu</th>\n",
              "      <th>GarageType</th>\n",
              "      <th>GarageYrBlt</th>\n",
              "      <th>GarageFinish</th>\n",
              "      <th>GarageCars</th>\n",
              "      <th>GarageArea</th>\n",
              "      <th>GarageQual</th>\n",
              "      <th>GarageCond</th>\n",
              "      <th>PavedDrive</th>\n",
              "      <th>WoodDeckSF</th>\n",
              "      <th>OpenPorchSF</th>\n",
              "      <th>EnclosedPorch</th>\n",
              "      <th>3SsnPorch</th>\n",
              "      <th>ScreenPorch</th>\n",
              "      <th>PoolArea</th>\n",
              "      <th>PoolQC</th>\n",
              "      <th>Fence</th>\n",
              "      <th>MiscFeature</th>\n",
              "      <th>MiscVal</th>\n",
              "      <th>MoSold</th>\n",
              "      <th>YrSold</th>\n",
              "      <th>SaleType</th>\n",
              "      <th>SaleCondition</th>\n",
              "      <th>SalePrice</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>65.0</td>\n",
              "      <td>8450</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Reg</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>Inside</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>CollgCr</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>2Story</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>2003</td>\n",
              "      <td>2003</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>BrkFace</td>\n",
              "      <td>196.0</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>PConc</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>No</td>\n",
              "      <td>GLQ</td>\n",
              "      <td>706</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>856</td>\n",
              "      <td>GasA</td>\n",
              "      <td>...</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>856</td>\n",
              "      <td>854</td>\n",
              "      <td>0</td>\n",
              "      <td>1710</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>Gd</td>\n",
              "      <td>8</td>\n",
              "      <td>Typ</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>2003.0</td>\n",
              "      <td>RFn</td>\n",
              "      <td>2</td>\n",
              "      <td>548</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>0</td>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>208500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>20</td>\n",
              "      <td>RL</td>\n",
              "      <td>80.0</td>\n",
              "      <td>9600</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Reg</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>FR2</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>Veenker</td>\n",
              "      <td>Feedr</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>1Story</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>1976</td>\n",
              "      <td>1976</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>MetalSd</td>\n",
              "      <td>MetalSd</td>\n",
              "      <td>None</td>\n",
              "      <td>0.0</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>CBlock</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>Gd</td>\n",
              "      <td>ALQ</td>\n",
              "      <td>978</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0</td>\n",
              "      <td>284</td>\n",
              "      <td>1262</td>\n",
              "      <td>GasA</td>\n",
              "      <td>...</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>1262</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1262</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>TA</td>\n",
              "      <td>6</td>\n",
              "      <td>Typ</td>\n",
              "      <td>1</td>\n",
              "      <td>TA</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>1976.0</td>\n",
              "      <td>RFn</td>\n",
              "      <td>2</td>\n",
              "      <td>460</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>298</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>2007</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>181500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>68.0</td>\n",
              "      <td>11250</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>Inside</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>CollgCr</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>2Story</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>2001</td>\n",
              "      <td>2002</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>BrkFace</td>\n",
              "      <td>162.0</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>PConc</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>Mn</td>\n",
              "      <td>GLQ</td>\n",
              "      <td>486</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0</td>\n",
              "      <td>434</td>\n",
              "      <td>920</td>\n",
              "      <td>GasA</td>\n",
              "      <td>...</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>920</td>\n",
              "      <td>866</td>\n",
              "      <td>0</td>\n",
              "      <td>1786</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>Gd</td>\n",
              "      <td>6</td>\n",
              "      <td>Typ</td>\n",
              "      <td>1</td>\n",
              "      <td>TA</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>2001.0</td>\n",
              "      <td>RFn</td>\n",
              "      <td>2</td>\n",
              "      <td>608</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>0</td>\n",
              "      <td>42</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>223500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>70</td>\n",
              "      <td>RL</td>\n",
              "      <td>60.0</td>\n",
              "      <td>9550</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>Corner</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>Crawfor</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>2Story</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>1915</td>\n",
              "      <td>1970</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>Wd Sdng</td>\n",
              "      <td>Wd Shng</td>\n",
              "      <td>None</td>\n",
              "      <td>0.0</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>BrkTil</td>\n",
              "      <td>TA</td>\n",
              "      <td>Gd</td>\n",
              "      <td>No</td>\n",
              "      <td>ALQ</td>\n",
              "      <td>216</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0</td>\n",
              "      <td>540</td>\n",
              "      <td>756</td>\n",
              "      <td>GasA</td>\n",
              "      <td>...</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>961</td>\n",
              "      <td>756</td>\n",
              "      <td>0</td>\n",
              "      <td>1717</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>Gd</td>\n",
              "      <td>7</td>\n",
              "      <td>Typ</td>\n",
              "      <td>1</td>\n",
              "      <td>Gd</td>\n",
              "      <td>Detchd</td>\n",
              "      <td>1998.0</td>\n",
              "      <td>Unf</td>\n",
              "      <td>3</td>\n",
              "      <td>642</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>0</td>\n",
              "      <td>35</td>\n",
              "      <td>272</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2006</td>\n",
              "      <td>WD</td>\n",
              "      <td>Abnorml</td>\n",
              "      <td>140000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>84.0</td>\n",
              "      <td>14260</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>FR2</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>NoRidge</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>2Story</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>2000</td>\n",
              "      <td>2000</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>BrkFace</td>\n",
              "      <td>350.0</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>PConc</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>Av</td>\n",
              "      <td>GLQ</td>\n",
              "      <td>655</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0</td>\n",
              "      <td>490</td>\n",
              "      <td>1145</td>\n",
              "      <td>GasA</td>\n",
              "      <td>...</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>1145</td>\n",
              "      <td>1053</td>\n",
              "      <td>0</td>\n",
              "      <td>2198</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>Gd</td>\n",
              "      <td>9</td>\n",
              "      <td>Typ</td>\n",
              "      <td>1</td>\n",
              "      <td>TA</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>RFn</td>\n",
              "      <td>3</td>\n",
              "      <td>836</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>192</td>\n",
              "      <td>84</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>250000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 81 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Id  MSSubClass MSZoning  ...  SaleType  SaleCondition SalePrice\n",
              "0   1          60       RL  ...        WD         Normal    208500\n",
              "1   2          20       RL  ...        WD         Normal    181500\n",
              "2   3          60       RL  ...        WD         Normal    223500\n",
              "3   4          70       RL  ...        WD        Abnorml    140000\n",
              "4   5          60       RL  ...        WD         Normal    250000\n",
              "\n",
              "[5 rows x 81 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrXVoqAU0h1g"
      },
      "source": [
        "Data preprocessing:\n",
        "* First I'll transform the skewed numeric features by taking log(feature + 1) - this will make the features more normal\n",
        "* Create Dummy variables for the categorical features\n",
        "* Replace the numeric missing values (NaN's) with the mean of their respective columns\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCCi1svksOj4"
      },
      "source": [
        "# Concatenate all the data\n",
        "# We do this to be able to preprocess on the whole dataset\n",
        "all_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],\n",
        "                      test.loc[:,'MSSubClass':'SaleCondition']))\n",
        "\n",
        "# Log transform the target y in training data - by reference inside all\n",
        "train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n",
        "\n",
        "# Log transform skewed numeric features:\n",
        "\n",
        "# Get Numerical Fields\n",
        "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index \n",
        "\n",
        "skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewnessc\n",
        "skewed_feats = skewed_feats[skewed_feats > 0.75] # Get Skewed Columns\n",
        "skewed_feats = skewed_feats.index # Get Skewed Columns indices\n",
        "\n",
        "# Log scale skewed columns\n",
        "# Normalize the skewed distribution for better regression\n",
        "all_data[skewed_feats] = np.log1p(all_data[skewed_feats])\n",
        "\n",
        "# Create Dummy variables for the categorical features \n",
        "all_data = pd.get_dummies(all_data) \n",
        "\n",
        "# Replace the numeric missing values (NaN's) with the mean of their respective columns\n",
        "all_data = all_data.fillna(all_data.mean())\n",
        "\n",
        "# Split the data to training & testing\n",
        "X_train = all_data[:train.shape[0]]\n",
        "X_test = all_data[train.shape[0]:]\n",
        "y = train.SalePrice\n",
        "\n",
        "# Standardize features by removing the mean and scaling to unit variance\n",
        "# z = (x - u) / s\n",
        "X_train = StandardScaler().fit_transform(X_train)\n",
        "\n",
        "#split training data into training & validation, default splitting is 25% validation\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y, random_state = 3)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVa9K1Ga1vlo"
      },
      "source": [
        "### 1.1.2.4 Define your model here (TODO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IocosMrh2W3X"
      },
      "source": [
        "One important note you need to be aware of, linear regression is a neural network with only one perceptron (i.e. dense layer with one node) with a linear activation (i.e. no activation function). \n",
        "\n",
        "![One Perceptron Neural Network](https://raw.githubusercontent.com/KhaledElTahan/AUC-DeepLearning/master/Labs/lab1/perceptron.png)\n",
        "\n",
        "Use this note to define a **sequential model of one dense layer with one unit using Tensorflow.Keras**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh1NvkNXsVVG"
      },
      "source": [
        "from tensorflow.keras import regularizers\n",
        "model = Sequential()# TODO: Define the Model using Tensorflow.Keras\n",
        "model.add(Dense(units=1, activation=\"linear\", input_shape=(X_tr.shape[1],), kernel_regularizer=l2(0.1), bias_regularizer=l2(0.1), name=\"dense_layer\"))"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSa178n-2BM-"
      },
      "source": [
        "### 1.1.2.5 Compile your model and print a summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fg6Yy5oDsXyK",
        "outputId": "6bd321e4-f665-4c65-d0d5-b43f03c06b05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.compile(loss = \"mean_squared_error\", optimizer = \"Adam\")\n",
        "model.summary()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_21\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_layer (Dense)          (None, 1)                 289       \n",
            "=================================================================\n",
            "Total params: 289\n",
            "Trainable params: 289\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FgD-Kqo3fwb"
      },
      "source": [
        "### 1.1.2.6 Train your model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aA4pLcN73waK"
      },
      "source": [
        "Fit your model into the training data, use the validation data to be able to plot the loss decrement during the training. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAkW1KTm3TGU",
        "outputId": "0d4f5a12-8e36-4483-d5db-b19784b5eb2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "hist = model.fit(X_tr, y_tr, validation_data = (X_val, y_val), epochs = 500)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "35/35 [==============================] - 1s 7ms/step - loss: 147.5517 - val_loss: 145.3256\n",
            "Epoch 2/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 145.7129 - val_loss: 146.1966\n",
            "Epoch 3/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 143.1315 - val_loss: 147.2393\n",
            "Epoch 4/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 143.1175 - val_loss: 148.3430\n",
            "Epoch 5/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 140.3054 - val_loss: 149.5427\n",
            "Epoch 6/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 139.8813 - val_loss: 150.8998\n",
            "Epoch 7/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 139.5602 - val_loss: 152.2088\n",
            "Epoch 8/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 136.9060 - val_loss: 153.6912\n",
            "Epoch 9/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 135.9796 - val_loss: 155.3300\n",
            "Epoch 10/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 134.8498 - val_loss: 156.9299\n",
            "Epoch 11/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 133.9353 - val_loss: 158.7280\n",
            "Epoch 12/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 131.3948 - val_loss: 160.4431\n",
            "Epoch 13/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 131.2836 - val_loss: 162.2081\n",
            "Epoch 14/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 128.8632 - val_loss: 164.0919\n",
            "Epoch 15/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 129.1343 - val_loss: 166.1130\n",
            "Epoch 16/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 127.0135 - val_loss: 168.0713\n",
            "Epoch 17/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 127.5364 - val_loss: 170.2107\n",
            "Epoch 18/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 125.7758 - val_loss: 172.4804\n",
            "Epoch 19/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 123.9124 - val_loss: 174.7751\n",
            "Epoch 20/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 122.6021 - val_loss: 176.8309\n",
            "Epoch 21/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 121.8711 - val_loss: 179.2643\n",
            "Epoch 22/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 122.6377 - val_loss: 181.5540\n",
            "Epoch 23/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 120.5808 - val_loss: 184.1077\n",
            "Epoch 24/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 119.6421 - val_loss: 186.5815\n",
            "Epoch 25/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 119.6775 - val_loss: 188.9937\n",
            "Epoch 26/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 117.1496 - val_loss: 191.5286\n",
            "Epoch 27/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 116.7464 - val_loss: 194.3147\n",
            "Epoch 28/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 115.4965 - val_loss: 196.8450\n",
            "Epoch 29/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 113.4337 - val_loss: 199.4152\n",
            "Epoch 30/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 111.8489 - val_loss: 202.1126\n",
            "Epoch 31/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 113.9013 - val_loss: 204.7274\n",
            "Epoch 32/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 112.4935 - val_loss: 207.5605\n",
            "Epoch 33/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 111.7634 - val_loss: 210.3560\n",
            "Epoch 34/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 110.3550 - val_loss: 212.9807\n",
            "Epoch 35/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 109.0845 - val_loss: 215.9505\n",
            "Epoch 36/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 107.9971 - val_loss: 218.5153\n",
            "Epoch 37/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 108.4370 - val_loss: 221.3060\n",
            "Epoch 38/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 106.8092 - val_loss: 224.1353\n",
            "Epoch 39/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 105.4519 - val_loss: 226.8379\n",
            "Epoch 40/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 104.0704 - val_loss: 229.7026\n",
            "Epoch 41/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 103.9558 - val_loss: 232.6309\n",
            "Epoch 42/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 104.7506 - val_loss: 235.6209\n",
            "Epoch 43/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 102.4815 - val_loss: 238.5300\n",
            "Epoch 44/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 101.1450 - val_loss: 241.4304\n",
            "Epoch 45/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 99.7450 - val_loss: 244.5290\n",
            "Epoch 46/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 99.1946 - val_loss: 247.3475\n",
            "Epoch 47/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 99.1246 - val_loss: 250.1415\n",
            "Epoch 48/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 98.9667 - val_loss: 253.0503\n",
            "Epoch 49/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 96.9775 - val_loss: 256.1941\n",
            "Epoch 50/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 97.1241 - val_loss: 258.7470\n",
            "Epoch 51/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 96.4243 - val_loss: 261.5876\n",
            "Epoch 52/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 97.0186 - val_loss: 264.6907\n",
            "Epoch 53/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 94.3164 - val_loss: 267.4211\n",
            "Epoch 54/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 96.1118 - val_loss: 270.1314\n",
            "Epoch 55/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 94.2307 - val_loss: 273.1734\n",
            "Epoch 56/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 92.8799 - val_loss: 276.2169\n",
            "Epoch 57/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 93.5841 - val_loss: 279.1893\n",
            "Epoch 58/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 92.5267 - val_loss: 282.0494\n",
            "Epoch 59/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 91.9355 - val_loss: 284.7639\n",
            "Epoch 60/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 92.6287 - val_loss: 287.3831\n",
            "Epoch 61/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 90.3324 - val_loss: 290.4117\n",
            "Epoch 62/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 88.1514 - val_loss: 293.2224\n",
            "Epoch 63/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 89.9746 - val_loss: 295.5069\n",
            "Epoch 64/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 88.1671 - val_loss: 298.1071\n",
            "Epoch 65/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 86.7807 - val_loss: 300.9695\n",
            "Epoch 66/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 87.4103 - val_loss: 303.3697\n",
            "Epoch 67/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 86.0091 - val_loss: 306.0168\n",
            "Epoch 68/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 85.3281 - val_loss: 308.4464\n",
            "Epoch 69/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 86.4794 - val_loss: 310.8452\n",
            "Epoch 70/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 85.3070 - val_loss: 313.5670\n",
            "Epoch 71/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 83.4160 - val_loss: 315.8927\n",
            "Epoch 72/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 82.8449 - val_loss: 318.2841\n",
            "Epoch 73/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 81.4406 - val_loss: 320.6871\n",
            "Epoch 74/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 83.2919 - val_loss: 322.7706\n",
            "Epoch 75/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 81.4488 - val_loss: 325.2325\n",
            "Epoch 76/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 80.2830 - val_loss: 327.4604\n",
            "Epoch 77/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 80.4740 - val_loss: 329.5483\n",
            "Epoch 78/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 79.3660 - val_loss: 331.4954\n",
            "Epoch 79/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 81.7337 - val_loss: 333.3864\n",
            "Epoch 80/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 79.5896 - val_loss: 335.3531\n",
            "Epoch 81/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 78.1280 - val_loss: 337.2626\n",
            "Epoch 82/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 77.9910 - val_loss: 339.0292\n",
            "Epoch 83/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 77.5833 - val_loss: 340.6467\n",
            "Epoch 84/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 78.1175 - val_loss: 342.3080\n",
            "Epoch 85/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 75.6924 - val_loss: 344.1825\n",
            "Epoch 86/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 75.6113 - val_loss: 345.7514\n",
            "Epoch 87/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 74.2679 - val_loss: 347.4592\n",
            "Epoch 88/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 75.7821 - val_loss: 348.7969\n",
            "Epoch 89/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 74.5535 - val_loss: 349.8103\n",
            "Epoch 90/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 74.9063 - val_loss: 351.3940\n",
            "Epoch 91/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 73.4136 - val_loss: 352.3957\n",
            "Epoch 92/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 74.7448 - val_loss: 353.6341\n",
            "Epoch 93/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 72.0773 - val_loss: 354.7293\n",
            "Epoch 94/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 71.9793 - val_loss: 355.5136\n",
            "Epoch 95/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 70.6069 - val_loss: 356.5478\n",
            "Epoch 96/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 71.1737 - val_loss: 357.4333\n",
            "Epoch 97/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 70.2190 - val_loss: 358.2672\n",
            "Epoch 98/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 69.7394 - val_loss: 358.9463\n",
            "Epoch 99/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 69.1342 - val_loss: 359.7474\n",
            "Epoch 100/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 67.6904 - val_loss: 359.9464\n",
            "Epoch 101/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 68.9116 - val_loss: 360.2846\n",
            "Epoch 102/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 67.2057 - val_loss: 360.7650\n",
            "Epoch 103/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 68.1457 - val_loss: 361.0012\n",
            "Epoch 104/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 68.5866 - val_loss: 361.1016\n",
            "Epoch 105/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 67.0529 - val_loss: 361.2397\n",
            "Epoch 106/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 66.5287 - val_loss: 361.5929\n",
            "Epoch 107/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 65.0201 - val_loss: 361.7138\n",
            "Epoch 108/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 66.5267 - val_loss: 361.2664\n",
            "Epoch 109/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 65.2242 - val_loss: 361.4468\n",
            "Epoch 110/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 65.0682 - val_loss: 360.6382\n",
            "Epoch 111/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 64.1353 - val_loss: 360.1936\n",
            "Epoch 112/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 64.4487 - val_loss: 359.6951\n",
            "Epoch 113/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 62.9390 - val_loss: 359.3582\n",
            "Epoch 114/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 62.2073 - val_loss: 358.9641\n",
            "Epoch 115/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 62.3634 - val_loss: 358.2328\n",
            "Epoch 116/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 60.3449 - val_loss: 357.7873\n",
            "Epoch 117/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 61.7013 - val_loss: 357.0335\n",
            "Epoch 118/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 62.0377 - val_loss: 356.2123\n",
            "Epoch 119/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 58.9312 - val_loss: 355.5237\n",
            "Epoch 120/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 60.9324 - val_loss: 354.4139\n",
            "Epoch 121/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 59.8005 - val_loss: 353.6931\n",
            "Epoch 122/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 59.1103 - val_loss: 352.8409\n",
            "Epoch 123/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 59.2669 - val_loss: 352.1013\n",
            "Epoch 124/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 58.6319 - val_loss: 351.1113\n",
            "Epoch 125/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 57.8023 - val_loss: 350.0064\n",
            "Epoch 126/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 57.6299 - val_loss: 348.8960\n",
            "Epoch 127/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 55.7790 - val_loss: 347.7123\n",
            "Epoch 128/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 56.0079 - val_loss: 346.5602\n",
            "Epoch 129/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 56.5281 - val_loss: 344.9970\n",
            "Epoch 130/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 55.7747 - val_loss: 343.7646\n",
            "Epoch 131/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 55.9393 - val_loss: 342.2794\n",
            "Epoch 132/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 54.8749 - val_loss: 341.0806\n",
            "Epoch 133/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 55.7288 - val_loss: 339.5708\n",
            "Epoch 134/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 53.9134 - val_loss: 338.2637\n",
            "Epoch 135/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 54.4189 - val_loss: 336.5725\n",
            "Epoch 136/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 54.3023 - val_loss: 335.0642\n",
            "Epoch 137/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 52.9264 - val_loss: 333.4637\n",
            "Epoch 138/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 53.4737 - val_loss: 331.8387\n",
            "Epoch 139/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 53.2522 - val_loss: 330.3923\n",
            "Epoch 140/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 52.3053 - val_loss: 328.6691\n",
            "Epoch 141/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 52.8017 - val_loss: 327.4074\n",
            "Epoch 142/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 51.4128 - val_loss: 325.4044\n",
            "Epoch 143/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 50.4312 - val_loss: 323.5476\n",
            "Epoch 144/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 52.1136 - val_loss: 321.5835\n",
            "Epoch 145/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 49.8122 - val_loss: 319.6552\n",
            "Epoch 146/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 49.5671 - val_loss: 318.0083\n",
            "Epoch 147/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 49.5452 - val_loss: 315.9639\n",
            "Epoch 148/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 49.0437 - val_loss: 314.2459\n",
            "Epoch 149/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 49.0304 - val_loss: 312.0643\n",
            "Epoch 150/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 48.6466 - val_loss: 310.3457\n",
            "Epoch 151/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 48.3766 - val_loss: 308.4324\n",
            "Epoch 152/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 47.5663 - val_loss: 306.2576\n",
            "Epoch 153/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 47.3014 - val_loss: 304.2695\n",
            "Epoch 154/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 48.3899 - val_loss: 302.1357\n",
            "Epoch 155/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 46.8512 - val_loss: 300.3680\n",
            "Epoch 156/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 45.8680 - val_loss: 298.4040\n",
            "Epoch 157/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 45.5767 - val_loss: 296.1872\n",
            "Epoch 158/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 46.6002 - val_loss: 294.0667\n",
            "Epoch 159/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 45.8926 - val_loss: 292.0988\n",
            "Epoch 160/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 45.4772 - val_loss: 289.9889\n",
            "Epoch 161/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 46.1006 - val_loss: 287.8465\n",
            "Epoch 162/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 44.0889 - val_loss: 285.9230\n",
            "Epoch 163/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 44.6771 - val_loss: 283.8818\n",
            "Epoch 164/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 43.5050 - val_loss: 281.4692\n",
            "Epoch 165/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 42.8467 - val_loss: 279.5315\n",
            "Epoch 166/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 42.1954 - val_loss: 277.4057\n",
            "Epoch 167/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 43.2793 - val_loss: 275.3693\n",
            "Epoch 168/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 43.4700 - val_loss: 273.2739\n",
            "Epoch 169/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 41.7424 - val_loss: 271.0993\n",
            "Epoch 170/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 42.4412 - val_loss: 269.0128\n",
            "Epoch 171/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 41.1805 - val_loss: 266.8117\n",
            "Epoch 172/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 40.9765 - val_loss: 264.9164\n",
            "Epoch 173/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 41.3170 - val_loss: 262.7721\n",
            "Epoch 174/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 40.9741 - val_loss: 260.3840\n",
            "Epoch 175/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 40.4994 - val_loss: 258.4361\n",
            "Epoch 176/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 39.9467 - val_loss: 256.3855\n",
            "Epoch 177/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 39.6943 - val_loss: 254.0424\n",
            "Epoch 178/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 39.2793 - val_loss: 251.8530\n",
            "Epoch 179/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 39.1465 - val_loss: 249.7694\n",
            "Epoch 180/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 39.1255 - val_loss: 247.6786\n",
            "Epoch 181/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 38.6151 - val_loss: 245.5638\n",
            "Epoch 182/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 38.8397 - val_loss: 243.5073\n",
            "Epoch 183/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 38.0209 - val_loss: 241.4352\n",
            "Epoch 184/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 37.0274 - val_loss: 239.3542\n",
            "Epoch 185/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 37.3911 - val_loss: 237.2985\n",
            "Epoch 186/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 36.9227 - val_loss: 235.1689\n",
            "Epoch 187/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 36.4400 - val_loss: 233.0024\n",
            "Epoch 188/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 36.3661 - val_loss: 230.9129\n",
            "Epoch 189/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 35.7603 - val_loss: 228.7846\n",
            "Epoch 190/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 35.9382 - val_loss: 226.9383\n",
            "Epoch 191/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 35.6162 - val_loss: 224.7570\n",
            "Epoch 192/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 35.2737 - val_loss: 222.8637\n",
            "Epoch 193/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 34.8498 - val_loss: 220.9422\n",
            "Epoch 194/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 34.4816 - val_loss: 218.8871\n",
            "Epoch 195/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 34.6752 - val_loss: 216.9273\n",
            "Epoch 196/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 34.4297 - val_loss: 214.7531\n",
            "Epoch 197/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 35.1411 - val_loss: 212.9720\n",
            "Epoch 198/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 34.0807 - val_loss: 210.7874\n",
            "Epoch 199/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 33.3848 - val_loss: 208.9855\n",
            "Epoch 200/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 33.6613 - val_loss: 207.2941\n",
            "Epoch 201/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 32.6863 - val_loss: 205.2912\n",
            "Epoch 202/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 32.8535 - val_loss: 203.3218\n",
            "Epoch 203/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 32.2507 - val_loss: 201.5283\n",
            "Epoch 204/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 32.6118 - val_loss: 199.6793\n",
            "Epoch 205/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 32.9359 - val_loss: 197.7140\n",
            "Epoch 206/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 31.6340 - val_loss: 195.5731\n",
            "Epoch 207/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 31.2367 - val_loss: 193.8245\n",
            "Epoch 208/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 31.6762 - val_loss: 192.0059\n",
            "Epoch 209/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 31.0377 - val_loss: 190.1996\n",
            "Epoch 210/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 30.5092 - val_loss: 188.2961\n",
            "Epoch 211/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 30.6809 - val_loss: 186.5506\n",
            "Epoch 212/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 30.2844 - val_loss: 184.6826\n",
            "Epoch 213/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 29.6775 - val_loss: 182.9963\n",
            "Epoch 214/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 30.1977 - val_loss: 181.2747\n",
            "Epoch 215/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 30.9276 - val_loss: 179.6927\n",
            "Epoch 216/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 28.4663 - val_loss: 177.7822\n",
            "Epoch 217/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 29.4642 - val_loss: 176.0781\n",
            "Epoch 218/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 28.9553 - val_loss: 174.1954\n",
            "Epoch 219/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 28.2541 - val_loss: 172.4819\n",
            "Epoch 220/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 28.2140 - val_loss: 170.6802\n",
            "Epoch 221/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 28.1868 - val_loss: 169.0062\n",
            "Epoch 222/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 28.6119 - val_loss: 167.4907\n",
            "Epoch 223/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 28.2089 - val_loss: 165.8391\n",
            "Epoch 224/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 28.7211 - val_loss: 164.3366\n",
            "Epoch 225/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 27.6514 - val_loss: 162.5947\n",
            "Epoch 226/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 27.2183 - val_loss: 160.8212\n",
            "Epoch 227/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 26.9788 - val_loss: 159.2507\n",
            "Epoch 228/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 26.9838 - val_loss: 157.5776\n",
            "Epoch 229/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 26.3668 - val_loss: 156.1243\n",
            "Epoch 230/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 26.1590 - val_loss: 154.6019\n",
            "Epoch 231/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 25.8710 - val_loss: 152.9736\n",
            "Epoch 232/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 26.3596 - val_loss: 151.4658\n",
            "Epoch 233/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 25.6644 - val_loss: 150.1131\n",
            "Epoch 234/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 25.3812 - val_loss: 148.6343\n",
            "Epoch 235/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 25.7336 - val_loss: 147.0885\n",
            "Epoch 236/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 25.1699 - val_loss: 145.3844\n",
            "Epoch 237/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 25.2475 - val_loss: 143.8237\n",
            "Epoch 238/500\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 24.9643 - val_loss: 142.2885\n",
            "Epoch 239/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 25.4053 - val_loss: 140.8616\n",
            "Epoch 240/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 24.4660 - val_loss: 139.0719\n",
            "Epoch 241/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 24.2628 - val_loss: 137.7471\n",
            "Epoch 242/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 24.3572 - val_loss: 136.4263\n",
            "Epoch 243/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 24.0637 - val_loss: 134.9696\n",
            "Epoch 244/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 23.2598 - val_loss: 133.6396\n",
            "Epoch 245/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 23.7909 - val_loss: 132.3747\n",
            "Epoch 246/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 23.0795 - val_loss: 130.7729\n",
            "Epoch 247/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 23.2331 - val_loss: 129.3328\n",
            "Epoch 248/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 23.0500 - val_loss: 128.0413\n",
            "Epoch 249/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 22.8795 - val_loss: 126.7425\n",
            "Epoch 250/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 22.4274 - val_loss: 125.3570\n",
            "Epoch 251/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 22.5650 - val_loss: 123.9170\n",
            "Epoch 252/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 22.1818 - val_loss: 122.3744\n",
            "Epoch 253/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 22.1431 - val_loss: 121.0863\n",
            "Epoch 254/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 21.9774 - val_loss: 119.6164\n",
            "Epoch 255/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 21.9753 - val_loss: 118.4637\n",
            "Epoch 256/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 22.0543 - val_loss: 117.0310\n",
            "Epoch 257/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 21.8659 - val_loss: 115.8142\n",
            "Epoch 258/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 21.5345 - val_loss: 114.5548\n",
            "Epoch 259/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 21.4384 - val_loss: 113.2800\n",
            "Epoch 260/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 21.6232 - val_loss: 112.0481\n",
            "Epoch 261/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 21.1217 - val_loss: 110.8903\n",
            "Epoch 262/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 21.1648 - val_loss: 109.7208\n",
            "Epoch 263/500\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 20.8454 - val_loss: 108.4585\n",
            "Epoch 264/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 20.8000 - val_loss: 107.1652\n",
            "Epoch 265/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 20.6422 - val_loss: 105.9398\n",
            "Epoch 266/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 20.1801 - val_loss: 104.6506\n",
            "Epoch 267/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 20.3612 - val_loss: 103.6221\n",
            "Epoch 268/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 19.9367 - val_loss: 102.4483\n",
            "Epoch 269/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 19.9121 - val_loss: 101.3001\n",
            "Epoch 270/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 20.1364 - val_loss: 100.1543\n",
            "Epoch 271/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 19.2393 - val_loss: 98.8539\n",
            "Epoch 272/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 19.6210 - val_loss: 97.9001\n",
            "Epoch 273/500\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 19.4539 - val_loss: 96.3923\n",
            "Epoch 274/500\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 19.1881 - val_loss: 95.5272\n",
            "Epoch 275/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 19.5499 - val_loss: 94.4199\n",
            "Epoch 276/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 18.8530 - val_loss: 93.1737\n",
            "Epoch 277/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 18.4519 - val_loss: 92.3058\n",
            "Epoch 278/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 18.7458 - val_loss: 91.3385\n",
            "Epoch 279/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 18.4797 - val_loss: 90.1043\n",
            "Epoch 280/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 18.6718 - val_loss: 89.1950\n",
            "Epoch 281/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 18.5352 - val_loss: 88.1579\n",
            "Epoch 282/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 18.6310 - val_loss: 87.0613\n",
            "Epoch 283/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 18.1023 - val_loss: 86.1110\n",
            "Epoch 284/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 18.0038 - val_loss: 84.9152\n",
            "Epoch 285/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 18.1215 - val_loss: 84.0499\n",
            "Epoch 286/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 17.6344 - val_loss: 82.9622\n",
            "Epoch 287/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 17.6967 - val_loss: 82.1601\n",
            "Epoch 288/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 17.5833 - val_loss: 81.1469\n",
            "Epoch 289/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 17.5953 - val_loss: 80.1977\n",
            "Epoch 290/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 17.5727 - val_loss: 79.2186\n",
            "Epoch 291/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 17.2323 - val_loss: 78.0615\n",
            "Epoch 292/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 17.4400 - val_loss: 77.3032\n",
            "Epoch 293/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 16.8667 - val_loss: 76.3800\n",
            "Epoch 294/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 17.0805 - val_loss: 75.5652\n",
            "Epoch 295/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 16.8308 - val_loss: 74.5981\n",
            "Epoch 296/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 16.9631 - val_loss: 73.6479\n",
            "Epoch 297/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 16.6151 - val_loss: 72.6315\n",
            "Epoch 298/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 16.6815 - val_loss: 71.8141\n",
            "Epoch 299/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 16.6511 - val_loss: 70.9624\n",
            "Epoch 300/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 16.6638 - val_loss: 70.2197\n",
            "Epoch 301/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 16.3880 - val_loss: 69.2394\n",
            "Epoch 302/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 16.2598 - val_loss: 68.2915\n",
            "Epoch 303/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 16.2835 - val_loss: 67.5982\n",
            "Epoch 304/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 16.0098 - val_loss: 66.7058\n",
            "Epoch 305/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 16.1709 - val_loss: 65.8686\n",
            "Epoch 306/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 15.9436 - val_loss: 65.0251\n",
            "Epoch 307/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 15.9119 - val_loss: 64.2166\n",
            "Epoch 308/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 15.8733 - val_loss: 63.4895\n",
            "Epoch 309/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 15.9122 - val_loss: 62.7283\n",
            "Epoch 310/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 15.5871 - val_loss: 61.9590\n",
            "Epoch 311/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 15.4523 - val_loss: 61.1846\n",
            "Epoch 312/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 15.6898 - val_loss: 60.6590\n",
            "Epoch 313/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 15.6738 - val_loss: 59.7508\n",
            "Epoch 314/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 15.4211 - val_loss: 58.9538\n",
            "Epoch 315/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 15.4373 - val_loss: 58.4013\n",
            "Epoch 316/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 15.1323 - val_loss: 57.5601\n",
            "Epoch 317/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 15.1712 - val_loss: 56.8143\n",
            "Epoch 318/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 15.0019 - val_loss: 56.1710\n",
            "Epoch 319/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 15.0462 - val_loss: 55.6302\n",
            "Epoch 320/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 15.1207 - val_loss: 54.7969\n",
            "Epoch 321/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 14.8543 - val_loss: 54.1651\n",
            "Epoch 322/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 15.2361 - val_loss: 53.5113\n",
            "Epoch 323/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 14.8572 - val_loss: 52.7290\n",
            "Epoch 324/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 14.8731 - val_loss: 52.1062\n",
            "Epoch 325/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 14.8526 - val_loss: 51.4764\n",
            "Epoch 326/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 14.5301 - val_loss: 50.8790\n",
            "Epoch 327/500\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 14.5441 - val_loss: 50.2687\n",
            "Epoch 328/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 14.5142 - val_loss: 49.6403\n",
            "Epoch 329/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 14.5767 - val_loss: 49.1825\n",
            "Epoch 330/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 14.6090 - val_loss: 48.4466\n",
            "Epoch 331/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 14.7692 - val_loss: 47.8962\n",
            "Epoch 332/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 14.2663 - val_loss: 47.2996\n",
            "Epoch 333/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 14.3922 - val_loss: 46.8507\n",
            "Epoch 334/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 14.3353 - val_loss: 46.2842\n",
            "Epoch 335/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 14.2959 - val_loss: 45.6906\n",
            "Epoch 336/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 14.3995 - val_loss: 45.1425\n",
            "Epoch 337/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 14.2641 - val_loss: 44.4837\n",
            "Epoch 338/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 14.0469 - val_loss: 44.0818\n",
            "Epoch 339/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 14.0646 - val_loss: 43.5567\n",
            "Epoch 340/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.9315 - val_loss: 43.1622\n",
            "Epoch 341/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.9548 - val_loss: 42.7560\n",
            "Epoch 342/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.9362 - val_loss: 42.0496\n",
            "Epoch 343/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.8961 - val_loss: 41.6745\n",
            "Epoch 344/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 14.0214 - val_loss: 41.0514\n",
            "Epoch 345/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.8371 - val_loss: 40.6187\n",
            "Epoch 346/500\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 13.7855 - val_loss: 40.2196\n",
            "Epoch 347/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.7275 - val_loss: 39.8256\n",
            "Epoch 348/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.6799 - val_loss: 39.3212\n",
            "Epoch 349/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.6997 - val_loss: 38.8602\n",
            "Epoch 350/500\n",
            "35/35 [==============================] - 0s 2ms/step - loss: 13.7620 - val_loss: 38.4197\n",
            "Epoch 351/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.7833 - val_loss: 37.9864\n",
            "Epoch 352/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.6150 - val_loss: 37.5400\n",
            "Epoch 353/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.6092 - val_loss: 37.1739\n",
            "Epoch 354/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.4992 - val_loss: 36.7018\n",
            "Epoch 355/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.5498 - val_loss: 36.2576\n",
            "Epoch 356/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.5426 - val_loss: 35.8937\n",
            "Epoch 357/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.4718 - val_loss: 35.4671\n",
            "Epoch 358/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.4721 - val_loss: 35.1552\n",
            "Epoch 359/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.4698 - val_loss: 34.8507\n",
            "Epoch 360/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.3399 - val_loss: 34.3974\n",
            "Epoch 361/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.2947 - val_loss: 34.1040\n",
            "Epoch 362/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.3337 - val_loss: 33.7120\n",
            "Epoch 363/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.3254 - val_loss: 33.3146\n",
            "Epoch 364/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.5747 - val_loss: 33.0741\n",
            "Epoch 365/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.3960 - val_loss: 32.6801\n",
            "Epoch 366/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.3640 - val_loss: 32.2712\n",
            "Epoch 367/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.2898 - val_loss: 32.0577\n",
            "Epoch 368/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.4107 - val_loss: 31.6770\n",
            "Epoch 369/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.3201 - val_loss: 31.3835\n",
            "Epoch 370/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.2669 - val_loss: 31.0024\n",
            "Epoch 371/500\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 13.3304 - val_loss: 30.8394\n",
            "Epoch 372/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.2024 - val_loss: 30.5107\n",
            "Epoch 373/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.2377 - val_loss: 30.2739\n",
            "Epoch 374/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.1888 - val_loss: 30.0370\n",
            "Epoch 375/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.1964 - val_loss: 29.6814\n",
            "Epoch 376/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.1466 - val_loss: 29.4797\n",
            "Epoch 377/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.2188 - val_loss: 29.1839\n",
            "Epoch 378/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.1495 - val_loss: 28.9468\n",
            "Epoch 379/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0715 - val_loss: 28.7092\n",
            "Epoch 380/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0935 - val_loss: 28.4388\n",
            "Epoch 381/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0979 - val_loss: 28.2482\n",
            "Epoch 382/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.1280 - val_loss: 28.0415\n",
            "Epoch 383/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.1603 - val_loss: 27.8122\n",
            "Epoch 384/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.1665 - val_loss: 27.5496\n",
            "Epoch 385/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0994 - val_loss: 27.2958\n",
            "Epoch 386/500\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 13.1900 - val_loss: 27.0634\n",
            "Epoch 387/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0918 - val_loss: 26.9263\n",
            "Epoch 388/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.1188 - val_loss: 26.7478\n",
            "Epoch 389/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.1128 - val_loss: 26.5683\n",
            "Epoch 390/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0811 - val_loss: 26.3323\n",
            "Epoch 391/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0746 - val_loss: 26.1793\n",
            "Epoch 392/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0484 - val_loss: 25.9621\n",
            "Epoch 393/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0819 - val_loss: 25.7874\n",
            "Epoch 394/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0245 - val_loss: 25.6330\n",
            "Epoch 395/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.1084 - val_loss: 25.5015\n",
            "Epoch 396/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0881 - val_loss: 25.3333\n",
            "Epoch 397/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0342 - val_loss: 25.1480\n",
            "Epoch 398/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0003 - val_loss: 25.0654\n",
            "Epoch 399/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9983 - val_loss: 24.9175\n",
            "Epoch 400/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0303 - val_loss: 24.8386\n",
            "Epoch 401/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9906 - val_loss: 24.6784\n",
            "Epoch 402/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0168 - val_loss: 24.5788\n",
            "Epoch 403/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0837 - val_loss: 24.3849\n",
            "Epoch 404/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9844 - val_loss: 24.1756\n",
            "Epoch 405/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0271 - val_loss: 24.1218\n",
            "Epoch 406/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0255 - val_loss: 24.0455\n",
            "Epoch 407/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0789 - val_loss: 23.9560\n",
            "Epoch 408/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9949 - val_loss: 23.8087\n",
            "Epoch 409/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0057 - val_loss: 23.7006\n",
            "Epoch 410/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0234 - val_loss: 23.6323\n",
            "Epoch 411/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0645 - val_loss: 23.5801\n",
            "Epoch 412/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9854 - val_loss: 23.4335\n",
            "Epoch 413/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0418 - val_loss: 23.3398\n",
            "Epoch 414/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9997 - val_loss: 23.2797\n",
            "Epoch 415/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0078 - val_loss: 23.1655\n",
            "Epoch 416/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9849 - val_loss: 23.0912\n",
            "Epoch 417/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0373 - val_loss: 23.0457\n",
            "Epoch 418/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0246 - val_loss: 22.9353\n",
            "Epoch 419/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0053 - val_loss: 22.8725\n",
            "Epoch 420/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9823 - val_loss: 22.8145\n",
            "Epoch 421/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9797 - val_loss: 22.7854\n",
            "Epoch 422/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0214 - val_loss: 22.6504\n",
            "Epoch 423/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9950 - val_loss: 22.6394\n",
            "Epoch 424/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9852 - val_loss: 22.5882\n",
            "Epoch 425/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0070 - val_loss: 22.5147\n",
            "Epoch 426/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9873 - val_loss: 22.4767\n",
            "Epoch 427/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9848 - val_loss: 22.3976\n",
            "Epoch 428/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0337 - val_loss: 22.3601\n",
            "Epoch 429/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9775 - val_loss: 22.3532\n",
            "Epoch 430/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0223 - val_loss: 22.2901\n",
            "Epoch 431/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0232 - val_loss: 22.2100\n",
            "Epoch 432/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9969 - val_loss: 22.1344\n",
            "Epoch 433/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9981 - val_loss: 22.1238\n",
            "Epoch 434/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0030 - val_loss: 22.0529\n",
            "Epoch 435/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9622 - val_loss: 22.0481\n",
            "Epoch 436/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9772 - val_loss: 22.0209\n",
            "Epoch 437/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9506 - val_loss: 22.0547\n",
            "Epoch 438/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0027 - val_loss: 21.9599\n",
            "Epoch 439/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9795 - val_loss: 21.9461\n",
            "Epoch 440/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9803 - val_loss: 21.8892\n",
            "Epoch 441/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9971 - val_loss: 21.8244\n",
            "Epoch 442/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9847 - val_loss: 21.8002\n",
            "Epoch 443/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0081 - val_loss: 21.7981\n",
            "Epoch 444/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9608 - val_loss: 21.7830\n",
            "Epoch 445/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9469 - val_loss: 21.7524\n",
            "Epoch 446/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9976 - val_loss: 21.7298\n",
            "Epoch 447/500\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 12.9669 - val_loss: 21.7266\n",
            "Epoch 448/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0210 - val_loss: 21.6277\n",
            "Epoch 449/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9681 - val_loss: 21.6925\n",
            "Epoch 450/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9591 - val_loss: 21.6334\n",
            "Epoch 451/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0129 - val_loss: 21.6203\n",
            "Epoch 452/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9856 - val_loss: 21.4705\n",
            "Epoch 453/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9912 - val_loss: 21.4823\n",
            "Epoch 454/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0279 - val_loss: 21.4448\n",
            "Epoch 455/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0045 - val_loss: 21.4114\n",
            "Epoch 456/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0220 - val_loss: 21.4202\n",
            "Epoch 457/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9671 - val_loss: 21.4423\n",
            "Epoch 458/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9974 - val_loss: 21.4337\n",
            "Epoch 459/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0276 - val_loss: 21.3309\n",
            "Epoch 460/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9967 - val_loss: 21.2905\n",
            "Epoch 461/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9798 - val_loss: 21.3271\n",
            "Epoch 462/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9838 - val_loss: 21.3401\n",
            "Epoch 463/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9601 - val_loss: 21.3383\n",
            "Epoch 464/500\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 12.9992 - val_loss: 21.3508\n",
            "Epoch 465/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0005 - val_loss: 21.3207\n",
            "Epoch 466/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9867 - val_loss: 21.2843\n",
            "Epoch 467/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9868 - val_loss: 21.2719\n",
            "Epoch 468/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9817 - val_loss: 21.2675\n",
            "Epoch 469/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9825 - val_loss: 21.2016\n",
            "Epoch 470/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9911 - val_loss: 21.2606\n",
            "Epoch 471/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0107 - val_loss: 21.2371\n",
            "Epoch 472/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9461 - val_loss: 21.2359\n",
            "Epoch 473/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9687 - val_loss: 21.2842\n",
            "Epoch 474/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9639 - val_loss: 21.2385\n",
            "Epoch 475/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9753 - val_loss: 21.2455\n",
            "Epoch 476/500\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 13.0098 - val_loss: 21.2001\n",
            "Epoch 477/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0161 - val_loss: 21.1737\n",
            "Epoch 478/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0133 - val_loss: 21.1895\n",
            "Epoch 479/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9273 - val_loss: 21.2366\n",
            "Epoch 480/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0217 - val_loss: 21.1799\n",
            "Epoch 481/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9631 - val_loss: 21.2033\n",
            "Epoch 482/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9723 - val_loss: 21.2037\n",
            "Epoch 483/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9483 - val_loss: 21.2137\n",
            "Epoch 484/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9740 - val_loss: 21.2177\n",
            "Epoch 485/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9930 - val_loss: 21.1526\n",
            "Epoch 486/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9590 - val_loss: 21.1084\n",
            "Epoch 487/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9760 - val_loss: 21.1445\n",
            "Epoch 488/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9760 - val_loss: 21.0772\n",
            "Epoch 489/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9998 - val_loss: 21.0782\n",
            "Epoch 490/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0322 - val_loss: 21.0422\n",
            "Epoch 491/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9873 - val_loss: 21.0481\n",
            "Epoch 492/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0047 - val_loss: 21.0680\n",
            "Epoch 493/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9722 - val_loss: 20.9963\n",
            "Epoch 494/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9812 - val_loss: 21.0726\n",
            "Epoch 495/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0016 - val_loss: 21.1174\n",
            "Epoch 496/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9917 - val_loss: 21.0980\n",
            "Epoch 497/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9846 - val_loss: 21.0521\n",
            "Epoch 498/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0041 - val_loss: 21.0442\n",
            "Epoch 499/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 13.0125 - val_loss: 20.9795\n",
            "Epoch 500/500\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 12.9690 - val_loss: 21.0226\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtdUbgUd4Ifw"
      },
      "source": [
        "And this is how you can predict an output for any number of inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaO-wCLl3Wd_",
        "outputId": "7a91ca2a-a0e2-437a-ad05-1d35d4e5b6b1"
      },
      "source": [
        "print(model.predict(X_test))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[13.606681]\n",
            " [10.274085]\n",
            " [13.147632]\n",
            " ...\n",
            " [13.514136]\n",
            " [ 8.284461]\n",
            " [14.415038]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6VhmPDN4Sva"
      },
      "source": [
        "### 1.1.2.7 Visualize Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bDCQffM4Zh2"
      },
      "source": [
        "It's time to see how your model's progress during the training, If all is good, you will find the validation loss decreasing without neither overfitting nor underfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEZ7SdwI2e_D",
        "outputId": "c2bec1d9-6360-4240-9606-6871d9899140",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "# Get training and test loss histories\n",
        "training_loss = hist.history['loss']\n",
        "val_loss = hist.history['val_loss']\n",
        "\n",
        "# Create count of the number of epochs\n",
        "epoch_count = range(1, len(training_loss) + 1)\n",
        "\n",
        "# Visualize loss history\n",
        "plt.figure()\n",
        "plt.plot(epoch_count, training_loss, 'r--')\n",
        "plt.plot(epoch_count, val_loss, 'b-')\n",
        "plt.legend(['Training Loss', 'Val Loss'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gni9AWDueGU"
      },
      "source": [
        "## 1.1.3 Conclusion:\n",
        "\n",
        "That's it! Congratulations on training a linear regression model. \n",
        "\n",
        "Make sure you finish the second part of the assignment and deliver all the requirements for the submission.\n",
        "\n"
      ]
    }
  ]
}